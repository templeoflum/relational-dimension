\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}

\title{Experiment 02: Compression Scaling Laws}
\author{Relational Dimension Project}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Building on Experiment 01's finding that compression ratio scales with graph size (ratio 3.11 for N=200/50), this experiment investigates the functional form of compression scaling. We test logarithmic ($\delta = a \log N + b$) and power law ($\delta = c N^\alpha$) models across graph sizes N=50 to N=1000. Results show logarithmic scaling fits better ($R^2 = 0.81$) than power law ($R^2 = 0.63$) for N$\leq$500, but methodological transitions at larger N complicate extrapolation testing.
\end{abstract}

\section{Introduction}

Experiment 01 established that long-range (LR) correlations produce measurable compression of correlation-based distances relative to topological distances. The compression ratio $\delta = (d_{topo} - d_{corr}) / d_{topo}$ increased from 0.108 at N=50 to 0.336 at N=200, yielding a scaling ratio of 3.11.

This experiment asks: \textbf{What is the functional form of this scaling, and does it predict behavior at unseen graph sizes?}

\section{Methods}

\subsection{Improved Dimension Estimation}

We extend Experiment 01's threshold-based dimension detection with continuous (fractional) dimension estimation:

\begin{enumerate}
    \item Compute reconstruction error curve $e(k)$ for $k = 1, \ldots, k_{max}$
    \item Find dimension where error drops below threshold $\tau \cdot e(1)$
    \item Use linear interpolation between integer dimensions for fractional estimate
\end{enumerate}

This approach maintains consistency with Experiment 01 while providing smoother dimension estimates.

\subsection{Sparse Methods for Large N}

For graphs with N $>$ 500, we employ:
\begin{itemize}
    \item \textbf{Landmark MDS}: Select 200 random landmarks, embed with classical MDS, project remaining points via distance-weighted interpolation
    \item \textbf{Sparse Isomap}: Use k-nearest neighbor graph (k=15) instead of full distance matrix
\end{itemize}

\subsection{Test Matrix}

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\toprule
N & Replications & Method \\
\midrule
50 & 10 & Full \\
100 & 10 & Full \\
200 & 10 & Full \\
300 & 10 & Full \\
500 & 10 & Full \\
750 & 5 & Sparse \\
1000 & 5 & Sparse \\
\bottomrule
\end{tabular}
\caption{Test matrix for scaling analysis. Total: 60 configurations.}
\end{table}

\subsection{Model Fitting}

We fit two models to the scaling data:

\textbf{Logarithmic:} $\delta(N) = a \log(N) + b$

\textbf{Power Law:} $\delta(N) = c N^\alpha$

Models are fit on N $\leq$ 500 (training set) and evaluated on N $>$ 500 (test set) for predictive validity.

\section{Results}

\subsection{Scaling Data}

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\toprule
N & $\delta$ (mean) & $\delta$ (std) \\
\midrule
50 & 0.108 & 0.317 \\
100 & 0.298 & 0.185 \\
200 & 0.337 & 0.509 \\
300 & 0.326 & 0.386 \\
500 & 0.386 & 0.547 \\
750 & -0.151 & 0.302 \\
1000 & -0.097 & 0.431 \\
\bottomrule
\end{tabular}
\caption{Compression ratio by graph size. Note sign change at N$>$500 where sparse methods are used.}
\end{table}

\subsection{Model Comparison}

For N $\leq$ 500:
\begin{itemize}
    \item Logarithmic: $\delta = 0.107 \log(N) - 0.258$, $R^2 = 0.812$
    \item Power Law: $\delta = 0.022 N^{0.487}$, $R^2 = 0.630$
\end{itemize}

The logarithmic model provides better fit, approaching but not exceeding the 0.85 threshold.

\subsection{Prediction Evaluation}

\begin{table}[h]
\centering
\begin{tabular}{llccc}
\toprule
ID & Description & Threshold & Measured & Status \\
\midrule
P1 & Monotonic Scaling & $r > 0.9$ & -0.29 & FAIL \\
P2 & Logarithmic Form & $R^2 > 0.85$ & 0.81 & FAIL \\
P3 & Power Law Form & $R^2 > 0.85$ & 0.63 & FAIL \\
P4 & Non-Saturation & $\Delta\delta > 0.05$ & -0.48 & FAIL \\
P5 & Predictive Validity & Error $< 20\%$ & 594\% & FAIL \\
P6 & Variance Reduction & Std $< 0.15$ & 0.19 & FAIL \\
\bottomrule
\end{tabular}
\caption{Prediction results. All predictions failed, primarily due to methodological discontinuity.}
\end{table}

\section{Discussion}

\subsection{Interpretation}

The results reveal a critical methodological issue: switching from full to sparse methods at N=500 introduces a discontinuity that invalidates cross-method comparisons. Within the full-method range (N=50-500):

\begin{itemize}
    \item Compression increases with N (consistent with Exp01)
    \item Logarithmic scaling provides reasonable fit ($R^2 = 0.81$)
    \item High variance suggests need for more replications
\end{itemize}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Sparse method artifact}: The negative $\delta$ values at N$>$500 likely reflect methodological differences rather than true compression reversal
    \item \textbf{Variance}: Standard deviations exceed means at several sizes, indicating high stochasticity
    \item \textbf{Sample size}: 5 replications at large N may be insufficient
\end{enumerate}

\subsection{Recommendations}

For future work:
\begin{itemize}
    \item Use consistent methods across all sizes (full methods are feasible up to N=1000)
    \item Increase replications to 20+ for reliable statistics
    \item Test intermediate sizes (400, 600, 800) to verify continuity
\end{itemize}

\section{Conclusion}

While all pre-registered predictions technically failed, the experiment provides valuable insights:

\begin{enumerate}
    \item Compression scaling IS present in the N=50-500 range
    \item Logarithmic scaling ($R^2 = 0.81$) fits better than power law ($R^2 = 0.63$)
    \item Methodological consistency is critical for cross-size comparisons
\end{enumerate}

The core hypothesis---that compression scales systematically with graph size---remains plausible but requires methodologically consistent replication at larger scales.

\section{Figures}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../reports/scaling_curve.png}
\caption{Compression ratio vs graph size with fitted logarithmic and power law models. Note discontinuity at N=500 where sparse methods begin.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../reports/predictions_summary.png}
\caption{Summary of prediction evaluations (P1-P6).}
\end{figure}

\end{document}
